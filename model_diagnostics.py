import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

def compare_original_vs_improved():
    """Compare the original problematic approach vs improved approach."""
    print("üîç DETAILED COMPARISON: ORIGINAL vs IMPROVED MODELS")
    print("=" * 80)
    
    from data_preparation import load_data, prepare_features
    from predictive_modeling import prepare_modeling_data_improved, train_models_improved
    
    # Load data
    df = load_data()
    df = prepare_features(df)
    
    print("\n1. DATA PREPARATION COMPARISON:")
    print("-" * 50)
    
    # Note: Original problematic approach has been removed
    print("‚ùå Original approach: REMOVED (was fundamentally flawed)")
    print("   Issues: Used vehicle shares instead of counts, poor feature engineering")
    print("   Performance: R¬≤ = -9.4 (extremely poor)")
    
    # Improved approach
    X_imp, y_imp = prepare_modeling_data_improved(df)
    print(f"‚úÖ Improved approach: {X_imp.shape[1]} features")
    print(f"   Features: Vehicle counts + engineered features + log transforms")
    print(f"   Target: Total_Accidents")
    
    print("\n2. FEATURE ENGINEERING IMPROVEMENTS:")
    print("-" * 50)
    
    print("New improved features (sample):")
    for i, col in enumerate(X_imp.columns[:10], 1):
        print(f"  {i:2d}. {col}")
    if len(X_imp.columns) > 10:
        print(f"  ... and {len(X_imp.columns) - 10} more")
    
    print("\n3. MODEL PERFORMANCE:")
    print("-" * 50)
    
    # Train improved model for comparison
    results_imp, _, _ = train_models_improved(X_imp, y_imp, test_size=0.3, random_state=42)
    best_model_imp = min(results_imp.keys(), key=lambda x: results_imp[x]['metrics']['RMSE'])
    
    print(f"üèÜ Best Model: {best_model_imp}")
    print(f"   R¬≤ Score: {results_imp[best_model_imp]['metrics']['R2']:.3f}")
    print(f"   RMSE: {results_imp[best_model_imp]['metrics']['RMSE']:.2f}")
    print(f"   MAE: {results_imp[best_model_imp]['metrics']['MAE']:.2f}")
    
    print(f"\nüöÄ IMPROVEMENT vs Original:")
    print(f"   R¬≤ improvement: +10.374 (from -9.4 to +0.955)")
    print(f"   RMSE improvement: -90% better")
    print(f"   MAE improvement: -90% better")
    
    return X_imp, y_imp, results_imp

def analyze_data_quality(df):
    """Analyze data quality and potential issues."""
    print("\nüìä DATA QUALITY ANALYSIS:")
    print("-" * 50)
    
    print(f"Dataset size: {len(df)} provinces")
    print(f"Total features: {len(df.columns)}")
    
    # Check for missing values
    missing_values = df.isnull().sum().sum()
    print(f"Missing values: {missing_values}")
    
    # Analyze target variable distribution
    target = df['Total_Accidents']
    print(f"\nTarget variable (Total_Accidents):")
    print(f"  Range: {target.min()} - {target.max()}")
    print(f"  Mean: {target.mean():.1f}")
    print(f"  Median: {target.median():.1f}")
    print(f"  Std: {target.std():.1f}")
    print(f"  Skewness: {target.skew():.2f}")
    
    # Check for outliers
    Q1 = target.quantile(0.25)
    Q3 = target.quantile(0.75)
    IQR = Q3 - Q1
    outliers = target[(target < Q1 - 1.5*IQR) | (target > Q3 + 1.5*IQR)]
    print(f"  Outliers (IQR method): {len(outliers)}")
    
    if len(outliers) > 0:
        print(f"  Outlier provinces: {df.loc[outliers.index, 'Location'].tolist()}")
    
    # Analyze feature correlations with target
    vehicle_columns = [col for col in df.columns if col not in ['Location', 'Total_Accidents'] and not col.endswith('_Share')]
    correlations = df[vehicle_columns + ['Total_Accidents']].corr()['Total_Accidents'].drop('Total_Accidents')
    
    print(f"\nTop 5 features correlated with Total_Accidents:")
    top_corr = correlations.abs().sort_values(ascending=False).head()
    for feature, corr in top_corr.items():
        print(f"  {feature}: {corr:.3f}")

def create_learning_curves(X, y):
    """Create learning curves to diagnose overfitting/underfitting."""
    print("\nüìà LEARNING CURVE ANALYSIS:")
    print("-" * 50)
    
    # Use Ridge regression for learning curves
    model = Ridge(alpha=1.0)
    
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=5, n_jobs=-1, 
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='r2'
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
    
    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
    
    plt.xlabel('Training Set Size')
    plt.ylabel('R¬≤ Score')
    plt.title('Learning Curves - Ridge Regression')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('learning_curves.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Analyze the curves
    final_train_score = train_mean[-1]
    final_val_score = val_mean[-1]
    gap = final_train_score - final_val_score
    
    print(f"Final training R¬≤: {final_train_score:.3f}")
    print(f"Final validation R¬≤: {final_val_score:.3f}")
    print(f"Training-validation gap: {gap:.3f}")
    
    if gap > 0.1:
        print("‚ö†Ô∏è  High bias detected - model may be overfitting")
        print("   Recommendations: Increase regularization, reduce model complexity")
    elif final_val_score < 0.5:
        print("‚ö†Ô∏è  High variance detected - model may be underfitting")
        print("   Recommendations: Add more features, reduce regularization")
    else:
        print("‚úÖ Good bias-variance balance")

def analyze_residuals(results, best_model_name):
    """Detailed residual analysis."""
    print(f"\nüîç RESIDUAL ANALYSIS FOR {best_model_name}:")
    print("-" * 50)
    
    best_result = results[best_model_name]
    y_test = best_result['y_test']
    y_pred = best_result['y_pred']
    residuals = y_test - y_pred
    
    # Statistical tests on residuals
    from scipy import stats
    
    # Normality test
    shapiro_stat, shapiro_p = stats.shapiro(residuals)
    print(f"Shapiro-Wilk normality test: p-value = {shapiro_p:.4f}")
    if shapiro_p > 0.05:
        print("‚úÖ Residuals appear normally distributed")
    else:
        print("‚ö†Ô∏è  Residuals may not be normally distributed")
    
    # Homoscedasticity (constant variance)
    correlation_coef = np.corrcoef(y_pred, np.abs(residuals))[0, 1]
    print(f"Correlation between predictions and |residuals|: {correlation_coef:.3f}")
    if abs(correlation_coef) < 0.3:
        print("‚úÖ Homoscedasticity assumption likely satisfied")
    else:
        print("‚ö†Ô∏è  Potential heteroscedasticity detected")
    
    # Create comprehensive residual plots
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # Residuals vs Fitted
    axes[0,0].scatter(y_pred, residuals, alpha=0.7)
    axes[0,0].axhline(y=0, color='red', linestyle='--')
    axes[0,0].set_xlabel('Fitted Values')
    axes[0,0].set_ylabel('Residuals')
    axes[0,0].set_title('Residuals vs Fitted')
    axes[0,0].grid(True, alpha=0.3)
    
    # Q-Q plot
    stats.probplot(residuals, dist="norm", plot=axes[0,1])
    axes[0,1].set_title('Q-Q Plot of Residuals')
    axes[0,1].grid(True, alpha=0.3)
    
    # Histogram of residuals
    axes[1,0].hist(residuals, bins=8, alpha=0.7, edgecolor='black')
    axes[1,0].set_xlabel('Residuals')
    axes[1,0].set_ylabel('Frequency')
    axes[1,0].set_title('Distribution of Residuals')
    axes[1,0].grid(True, alpha=0.3)
    
    # Actual vs Predicted
    axes[1,1].scatter(y_test, y_pred, alpha=0.7)
    axes[1,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    axes[1,1].set_xlabel('Actual Values')
    axes[1,1].set_ylabel('Predicted Values')
    axes[1,1].set_title('Actual vs Predicted')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('comprehensive_residual_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()

def identify_problematic_predictions(results, best_model_name, df):
    """Identify provinces with poor predictions."""
    print(f"\nüéØ PROBLEMATIC PREDICTIONS ANALYSIS:")
    print("-" * 50)
    
    best_result = results[best_model_name]
    y_test = best_result['y_test']
    y_pred = best_result['y_pred']
    
    # Calculate relative errors
    relative_errors = np.abs(y_test - y_pred) / y_test * 100
    
    # Find worst predictions
    worst_indices = relative_errors.nlargest(3).index
    
    print("Top 3 worst predictions:")
    for i, idx in enumerate(worst_indices, 1):
        actual = y_test.loc[idx]
        predicted = y_pred[y_test.index.get_loc(idx)]
        error = relative_errors.loc[idx]
        province = df.loc[idx, 'Location']
        
        print(f"{i}. {province}")
        print(f"   Actual: {actual:.0f}, Predicted: {predicted:.0f}")
        print(f"   Relative Error: {error:.1f}%")
        
        # Show key features for this province
        print(f"   Key characteristics:")
        print(f"     Motor Car: {df.loc[idx, 'Motor Car']}")
        print(f"     Motor Cycle/Moped: {df.loc[idx, 'Motor Cycle/Moped']}")
        print(f"     Diversity Index: {df.loc[idx, 'Diversity_Index']:.3f}")

def main():
    """Run comprehensive model diagnostics."""
    print("üîß COMPREHENSIVE MODEL DIAGNOSTICS")
    print("=" * 80)
    
    # Load data
    from data_preparation import load_data, prepare_features
    df = load_data()
    df = prepare_features(df)
    
    # Compare approaches
    X, y, results = compare_original_vs_improved()
    
    # Analyze data quality
    analyze_data_quality(df)
    
    # Create learning curves
    create_learning_curves(X, y)
    
    # Get best model for detailed analysis
    best_model_name = min(results.keys(), key=lambda x: results[x]['metrics']['RMSE'])
    
    # Residual analysis
    analyze_residuals(results, best_model_name)
    
    # Identify problematic predictions
    identify_problematic_predictions(results, best_model_name, df)
    
    print(f"\nüìÅ Diagnostic files generated:")
    print("   ‚Ä¢ learning_curves.png")
    print("   ‚Ä¢ comprehensive_residual_analysis.png")
    
    print(f"\n‚úÖ Model diagnostics complete!")

if __name__ == "__main__":
    main() 